{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "># **Ciência dos Dados** <font color=green>- Projeto 2</font>\n",
    "   #### <font color=grey> Beatriz Mie Kotsubo Kuwabara, <p>Lucas Nicascio dos Santos,</p>Manuela Castilla Russo Correa <p> </font>\n",
    "\n",
    "## Classificador Automático de Sentimento com Vários Níveis de Relevância"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classificação de informação se dá pela categorização de uma grande base de dados. Atualmente, uma grande fonte de dados e consequentemente de informações que ajudam a compreender comportamentos e tendências da sociedade são as redes sociais, como o Twitter, uma das mais populares e tornou-se uma importante plataforma para uma variedade de tarefas, incluindo a predição\n",
    "ção dos resultados eleitorais.\n",
    "\n",
    "<p>Nesse projeto, utilizando-se da API do Twitter, tivemos acesso a alguns recentes tweets publicados em inglês na plataforma de forma a analisar as reações dos usuários à marca Nike ultimamente dado o lançamento de uma recente campanha protagonizada pelo ex-jogador da NFL Colin Kaepernick, gerando reações contra e a favor à marca. Na base da dados foi classificado se os tweets eram muito irrelevantes, irrelevantes, neutro, relevantes ou muito relevantes. </p>\n",
    "\n",
    "<p>A implementação do \"machine learning\" por meio do classificador Naive Bayes utiliza-se de uma base de 300 tweets para o treinamento do código para uma posterior verificação com diferentes tweets em uma base de teste.</p>\n",
    "\n",
    "<p>O classificador Naive Bayes é um algoritmo para a tarefa de classificação utilizando o teorema de Bayes, que é uma equação que descreve a relação de probabilidades condicionais de grandezas estatísticas. Pode por exemplo ser utilizado na categorização de notícias ao analizar seus textos individualmente por conta da frequência em que algumas palavras aparecem, ou de forma mais avançada para reconhecimento facial ao analisar distribuições de probabilidades de pixels de certas cores. Mas o classificador fornece melhores resultados quando o usamos para análise de dados textuais.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCEDIMENTOS PARA A CRIAÇÃO DO CÓDIGO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Limpamos os tweets\n",
    "##### Para poder mexer e manipular os dados, tiramos as pontuções, palavras de duas letras (já que uma palavra tão pequena não será irrelevante em relação ao sentimento dos usuários). Além disso, separamos os emojis das palavras:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente importamos para poder realizar o código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as math\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from emoji import UNICODE_EMOJI\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import sub\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemos os arquivos do excel:\n",
    "\n",
    "Criamos três colunas na qual seriam:\n",
    "\n",
    "(1) Os tweets \n",
    "\n",
    "(2) Classificação apenas entre relevante ou irrelevante\n",
    "\n",
    "(2) Classificação entre os níveis de sentimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>Valor_(R(1)_I(0))</th>\n",
       "      <th>Níveis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>save big with joyhunt-up to 80% off rpp on gre...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rt @virgilableaux: hi. former us navy sailor h...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Treinamento  Valor_(R(1)_I(0))  \\\n",
       "0  save big with joyhunt-up to 80% off rpp on gre...                  1   \n",
       "1  rt @virgilableaux: hi. former us navy sailor h...                  1   \n",
       "\n",
       "   Níveis  \n",
       "0       2  \n",
       "1       4  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nike = pd.read_excel('1537437860647_tweets_nike_201809042210.xlsx', sheet_name = 0)\n",
    "nike_treinamento = nike[\"Treinamento\"]\n",
    "nike.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>Níveis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>save big with joyhuntup to 80 off rpp on great...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rt virgilableaux hi former us navy sailor here...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boycottnike i think a great big thank you nike...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt thedonholly if you plan on boycotting nike ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rt thedonholly if you plan on boycotting nike ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  Níveis\n",
       "0  save big with joyhuntup to 80 off rpp on great...       2\n",
       "1  rt virgilableaux hi former us navy sailor here...       4\n",
       "2  boycottnike i think a great big thank you nike...       4\n",
       "3  rt thedonholly if you plan on boycotting nike ...       3\n",
       "4  rt thedonholly if you plan on boycotting nike ...       3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "pontu = string.punctuation\n",
    "\n",
    "#criação de lista para tirar os tabs e os enters\n",
    "pont =[\"\\n\", \"\\t\"]\n",
    "\n",
    "#simplificação para pegar o dataframde dos tweets\n",
    "tweet = nike[\"Treinamento\"]\n",
    "\n",
    "### limpando os tweets\n",
    "tweets_limpos = []\n",
    "\n",
    "for frase in tweet:\n",
    "    x = \"\"\n",
    "    for m in frase:\n",
    "        if m in UNICODE_EMOJI:\n",
    "            x = x + \" \" + m + \" \"\n",
    "        elif m in pont:\n",
    "            x += \" \"\n",
    "        elif m not in pontu:\n",
    "            x += m\n",
    "    tweets_limpos.append(x)\n",
    "\n",
    "# criando um dataframe com os tweets limpos\n",
    "nike_limpinho = pd.DataFrame()\n",
    "nike_limpinho[\"tweets\"]= tweets_limpos\n",
    "nike_limpinho[\"Níveis\"] = nike[\"Níveis\"]\n",
    "\n",
    "nike_limpinho.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>Níveis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>save big with joyhuntup off rpp great brands s...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>virgilableaux former navy sailor here wonderin...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boycottnike think great big thank you nike fro...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thedonholly you plan boycotting nike will disp...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thedonholly you plan boycotting nike will disp...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  Níveis\n",
       "0  save big with joyhuntup off rpp great brands s...       2\n",
       "1  virgilableaux former navy sailor here wonderin...       4\n",
       "2  boycottnike think great big thank you nike fro...       4\n",
       "3  thedonholly you plan boycotting nike will disp...       3\n",
       "4  thedonholly you plan boycotting nike will disp...       3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## limpar os nomes:\n",
    "    # tirar tudo que é irrelevante para minha pesquisa e para descobrir a probabilidade como: @,\n",
    "    # palavras com poucas letras, #, link(site).\n",
    "\n",
    "tweet_limpos =[]\n",
    "y = \" \"\n",
    "for tweet in nike_limpinho[\"tweets\"]:\n",
    "    limpao = [] \n",
    "    splitei = tweet.split(\" \")\n",
    "    for palavra in splitei:\n",
    "        if palavra in UNICODE_EMOJI:\n",
    "            limpao.append(palavra)\n",
    "        elif len(palavra)> 2 and palavra[0]!=\"@\" and palavra[0]!=\"#\" and palavra[:4] !=\"http\":\n",
    "            limpao.append(palavra)\n",
    "    tweet_limpos.append(y.join(limpao))\n",
    "    \n",
    "nike_limpao = pd.DataFrame()\n",
    "nike_limpao[\"tweets\"]= tweet_limpos\n",
    "nike_limpao[\"Níveis\"] = nike[\"Níveis\"]\n",
    "\n",
    "nike_limpao.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Preparando os dados para implementação de Bayes\n",
    "##### Primeiramente criamos uma lista em que selecionava todas as palavras sem repetição, tendo um total de palavras em existentes em todos os tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras = []\n",
    "for e in nike_limpao[\"tweets\"]: \n",
    "    y = e.split()\n",
    "    for m in y:\n",
    "        if m not in palavras: \n",
    "            palavras.append(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Depois disso, criamos um contador de cada nível em que contava o número de vezes que aparecia tweets (palavras) em relevantes e irrelevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIrrel = 0\n",
    "Irrel = 0\n",
    "Neutro = 0\n",
    "Rel=0\n",
    "MRel = 0\n",
    "for i in range(len(nike_limpao)):\n",
    "    linha = nike_limpao[\"tweets\"][i].split(\" \")\n",
    "    for m in linha:\n",
    "        if nike_limpao[\"Níveis\"][i] == 0:\n",
    "            MIrrel +=1\n",
    "        elif nike_limpao[\"Níveis\"][i]== 1:\n",
    "            Irrel+=1\n",
    "        elif nike_limpao[\"Níveis\"][i]== 2:\n",
    "            Neutro+=1\n",
    "        elif nike_limpao[\"Níveis\"][i]== 3:\n",
    "            Rel+=1\n",
    "        else:\n",
    "            MRel +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Após isso, calculamos a frequência com a qual cada palavra aparece em cada nível de classificação\n",
    "\n",
    "Para isso, criamos um dicionário. Depois disso, fizemos um for para que passasse em cada palavra de cada tweet adicionando a palavra pelo menos uma vez em cada dicionário (já iniciando o laplace smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_MR = {}\n",
    "freq_R = {}\n",
    "freq_MI = {}\n",
    "freq_I = {}\n",
    "freq_N = {}\n",
    "\n",
    "for palavra in palavras:\n",
    "    freq_MR[palavra] = 1\n",
    "    freq_R[palavra] = 1\n",
    "    freq_MI[palavra] = 1\n",
    "    freq_I[palavra] = 1\n",
    "    freq_N[palavra] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Depois de adicionar pelo menos uma vez a palavra em cada dicionário, passamos novamente pelas palavras,mas agora adicionando nos dicionários de acordo o nível em que a palavra estava em um tweet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(nike_limpao)):\n",
    "    palavra = nike_limpao[\"tweets\"][i].split(\" \")\n",
    "    for m in palavra:\n",
    "        if nike_limpao[\"Níveis\"][i]== 0:\n",
    "            freq_MI[m]+=1\n",
    "        elif nike_limpao[\"Níveis\"][i]== 1:\n",
    "            freq_I[m]+=1\n",
    "        elif nike_limpao[\"Níveis\"][i]== 2:\n",
    "            freq_N[m]+=1\n",
    "        elif nike_limpao[\"Níveis\"][i]== 3:\n",
    "            freq_R[m]+=1\n",
    "        else:\n",
    "            freq_MR[m]+=1\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Implementando a Naive-Bayes\n",
    "\n",
    "#### Primeiramente calculamos a probabilidade de uma palavra sabendo seu nível:\n",
    "- P(palavra|Muito relevante)\n",
    "- P(palavra|Relevante)\n",
    "- P(palavra|Neutro)\n",
    "- P(palavra|Irrelevante)\n",
    "- P(palavra|Muito irrelevante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probabilidade de ser relevante dada cada palavra\n",
    "\n",
    "prob_mi ={}\n",
    "prob_i ={}\n",
    "prob_n ={}\n",
    "prob_r = {}\n",
    "prob_mr ={}\n",
    "\n",
    "for palavra in palavras:\n",
    "    prob_mi[palavra]= freq_MI[palavra]/(len(palavras)+MIrrel)\n",
    "    prob_i[palavra]= freq_I[palavra]/(len(palavras)+Irrel)\n",
    "    prob_n[palavra]= freq_N[palavra]/(len(palavras)+Neutro)\n",
    "    prob_r[palavra]= freq_R[palavra]/(len(palavras)+Rel)\n",
    "    prob_mr[palavra]= freq_MR[palavra]/(len(palavras)+MRel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Calculamos agora a probabilidade de ser relevante dado cada tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "m_irrelevante = 0\n",
    "irrelevante = 0\n",
    "neutro=0\n",
    "relevante = 0\n",
    "m_relevante =0\n",
    "\n",
    "for p in nike_limpao[\"Níveis\"]:\n",
    "    if p ==0:\n",
    "        m_irrelevante+=1\n",
    "    elif p ==1:\n",
    "        irrelevante+=1\n",
    "    elif p ==2:\n",
    "        neutro+=1\n",
    "    elif p ==3:\n",
    "        relevante+=1\n",
    "    else:\n",
    "        m_relevante +=1\n",
    "        \n",
    "pMI = m_irrelevante/len(nike_limpao[\"Níveis\"])\n",
    "pI = irrelevante/len(nike_limpao[\"Níveis\"])\n",
    "pN = neutro/len(nike_limpao[\"Níveis\"])\n",
    "pR = relevante/len(nike_limpao[\"Níveis\"])\n",
    "pMR = m_relevante/len(nike_limpao[\"Níveis\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Iniciando com o Teste\n",
    "### Limpeza dos tweets\n",
    "##### Agora iniciamos com a parte Teste. Para isso, é preciso realizar o mesmo processo de limpeza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lendo a segunda página para ir ao teste\n",
    "teste = pd.read_excel('1537437860647_tweets_nike_201809042210.xlsx', sheet_name = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>Níveis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt jamiesundays nike looking at the white peop...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trump weighs in on nikes kaepernick deal i thi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rt rafaelshimunov im finally cutting out the n...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dear nike i am not going to boycott you becaus...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rt lindsaytuten anyone who doesn’t want their ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  Níveis\n",
       "0  rt jamiesundays nike looking at the white peop...       4\n",
       "1  trump weighs in on nikes kaepernick deal i thi...       3\n",
       "2  rt rafaelshimunov im finally cutting out the n...       3\n",
       "3  dear nike i am not going to boycott you becaus...       3\n",
       "4  rt lindsaytuten anyone who doesn’t want their ...       3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#criação de lista para tirar os tabs e os enters\n",
    "pont =[\"\\n\", \"\\t\"]\n",
    "\n",
    "#simplificação para pegar o dataframde dos tweets\n",
    "tweet = teste[\"Teste\"]\n",
    "\n",
    "### limpando os tweets\n",
    "tweets_limpos = []\n",
    "\n",
    "for frase in tweet:\n",
    "    x = \"\"\n",
    "    for m in frase:\n",
    "        if m in UNICODE_EMOJI:\n",
    "            x = x + \" \" + m + \" \"\n",
    "        elif m in pont:\n",
    "            x += \" \"\n",
    "        elif m not in pontu:\n",
    "            x += m\n",
    "    tweets_limpos.append(x)\n",
    "\n",
    "# criando um dataframe com os tweets limpos\n",
    "nike_limpinho = pd.DataFrame()\n",
    "nike_limpinho[\"tweets\"]= tweets_limpos\n",
    "nike_limpinho[\"Níveis\"] = teste[\"Níveis\"]\n",
    "\n",
    "nike_limpinho.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>Níveis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jamiesundays nike looking the white people des...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trump weighs nikes kaepernick deal think its t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rafaelshimunov finally cutting out the nike lo...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dear nike not going boycott you because just s...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lindsaytuten anyone who doesn’t want their nik...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  Níveis\n",
       "0  jamiesundays nike looking the white people des...       4\n",
       "1  trump weighs nikes kaepernick deal think its t...       3\n",
       "2  rafaelshimunov finally cutting out the nike lo...       3\n",
       "3  dear nike not going boycott you because just s...       3\n",
       "4  lindsaytuten anyone who doesn’t want their nik...       3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# código para fazer o programa classificar os tweets a partir da probabilidade\n",
    "\n",
    "tweet_limpos =[]\n",
    "y = \" \"\n",
    "for tweet in nike_limpinho[\"tweets\"]:\n",
    "    limpao = [] \n",
    "    splitei = tweet.split(\" \")\n",
    "    for palavra in splitei:\n",
    "        if palavra in UNICODE_EMOJI:\n",
    "            limpao.append(palavra)\n",
    "        elif len(palavra)> 2 and palavra[0]!=\"@\" and palavra[0]!=\"#\" and palavra[:4] !=\"http\":\n",
    "            limpao.append(palavra)\n",
    "    tweet_limpos.append(y.join(limpao))\n",
    "    \n",
    "nike_limpao = pd.DataFrame()\n",
    "nike_limpao[\"tweets\"]= tweet_limpos\n",
    "nike_limpao[\"Níveis\"] = teste[\"Níveis\"]\n",
    "\n",
    "nike_limpao.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Finalizando a Naive Bayes\n",
    "##### Por fim, juntamos os dados fazendo então o produto de P(frase|rel) com P(Rel), criando então o programa na qual este estima o nível do tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>Níveis</th>\n",
       "      <th>Chute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jamiesundays nike looking the white people des...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trump weighs nikes kaepernick deal think its t...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rafaelshimunov finally cutting out the nike lo...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dear nike not going boycott you because just s...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lindsaytuten anyone who doesn’t want their nik...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  Níveis  Chute\n",
       "0  jamiesundays nike looking the white people des...       4      3\n",
       "1  trump weighs nikes kaepernick deal think its t...       3      4\n",
       "2  rafaelshimunov finally cutting out the nike lo...       3      4\n",
       "3  dear nike not going boycott you because just s...       3      3\n",
       "4  lindsaytuten anyone who doesn’t want their nik...       3      3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fazendo P(frase|rel) * P(Rel)\n",
    "chute = []\n",
    "\n",
    "for frase in nike_limpao[\"tweets\"]:\n",
    "    pMI_t = 1\n",
    "    pI_t = 1\n",
    "    pN_t = 1\n",
    "    pR_t= 1\n",
    "    pMR_t = 1\n",
    "    y = frase.split(\" \")\n",
    "    for palavra in y:\n",
    "        \n",
    "        if palavra in prob_mi:\n",
    "            pMI_t *= prob_mi[palavra]\n",
    "        else:\n",
    "            pMI_t *= 1/ (len(palavras)+MIrrel)\n",
    "        \n",
    "        if palavra in prob_i:\n",
    "            pI_t *= prob_i[palavra]\n",
    "        else:\n",
    "            pI_t *= 1/ (len(palavras)+Irrel)\n",
    "            \n",
    "        if palavra in prob_n:\n",
    "            pN_t *= prob_n[palavra]\n",
    "        else:\n",
    "            pN_t *= 1/ (len(palavras)+Neutro)\n",
    "            \n",
    "        if palavra in prob_r:\n",
    "            pR_t*= prob_r[palavra]\n",
    "        else:\n",
    "            pR_t *= 1/(len(palavra)+Rel)\n",
    "            \n",
    "        if palavra in prob_mr:\n",
    "            pMR_t *= prob_mr[palavra]\n",
    "        else:\n",
    "            pMR_t *= 1/ (len(palavras)+MRel)\n",
    "\n",
    "  \n",
    "    PMIR = (pMI*pMI_t)\n",
    "    PIR =(pI*pI_t)\n",
    "    PN = (pN*pN_t)\n",
    "    PR =( pR*pR_t)\n",
    "    PMR = (pMR*pMR_t)\n",
    "    \n",
    "    if PMR >= PR and PMR>=PN and PMR>=PIR and PMR>=PMIR:\n",
    "        chute.append(4)\n",
    "        \n",
    "    elif PR>=PN and PR>=PIR and PR>=PMIR:\n",
    "        chute.append(3)\n",
    "        \n",
    "    elif PN>=PIR and PN>=PMIR:\n",
    "        chute.append(2)\n",
    "        \n",
    "    elif PIR>=PMIR:\n",
    "        chute.append(1)\n",
    "        \n",
    "    else:\n",
    "        chute.append(0)\n",
    "        \n",
    "        \n",
    "# adicionando essa coluna ao nosso dataframe\n",
    "\n",
    "nike_limpao[\"Chute\"] = chute\n",
    "nike_limpao.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparando a classificação do teste e do treinamento\n",
    "\n",
    "zero = nike_limpao.loc[(nike_limpao[\"Níveis\"]==0)&(nike_limpao[\"Chute\"]==0), [\"Níveis\", \"Chute\"]]\n",
    "zeroC = nike_limpao.loc[(nike_limpao[\"Níveis\"]!=0)&(nike_limpao[\"Chute\"]==0), [\"Níveis\", \"Chute\"]]\n",
    "\n",
    "um = nike_limpao.loc[(nike_limpao[\"Níveis\"]==1)&(nike_limpao[\"Chute\"]==1), [\"Níveis\", \"Chute\"]]\n",
    "umC = nike_limpao.loc[(nike_limpao[\"Níveis\"]!=1)&(nike_limpao[\"Chute\"]==1), [\"Níveis\", \"Chute\"]]\n",
    "\n",
    "dois = nike_limpao.loc[(nike_limpao[\"Níveis\"]==2)&(nike_limpao[\"Chute\"]==2), [\"Níveis\", \"Chute\"]]\n",
    "doisC = nike_limpao.loc[(nike_limpao[\"Níveis\"]!=2)&(nike_limpao[\"Chute\"]==2), [\"Níveis\", \"Chute\"]]\n",
    "\n",
    "tres = nike_limpao.loc[(nike_limpao[\"Níveis\"]==3)&(nike_limpao[\"Chute\"]==3), [\"Níveis\", \"Chute\"]]\n",
    "tresC= nike_limpao.loc[(nike_limpao[\"Níveis\"]!=3)&(nike_limpao[\"Chute\"]==3), [\"Níveis\", \"Chute\"]]\n",
    "\n",
    "quatro= nike_limpao.loc[(nike_limpao[\"Níveis\"]==4)&(nike_limpao[\"Chute\"]==4), [\"Níveis\", \"Chute\"]]\n",
    "quatroC = nike_limpao.loc[(nike_limpao[\"Níveis\"]!=4)&(nike_limpao[\"Chute\"]==4), [\"Níveis\", \"Chute\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  4  0]\n",
      " [ 0  0  0  2  0]\n",
      " [ 0  0  2  5  0]\n",
      " [ 0  0 11 91 47]\n",
      " [ 0  0  0 27 11]]\n"
     ]
    }
   ],
   "source": [
    "#fazendo uma matriz de confusão\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(nike_limpao[\"Níveis\"], nike_limpao[\"Chute\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predito</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Real</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.055</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>0.065</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.290</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predito      2      3      4    All\n",
       "Real                               \n",
       "0        0.000  0.020  0.000  0.020\n",
       "1        0.000  0.010  0.000  0.010\n",
       "2        0.010  0.025  0.000  0.035\n",
       "3        0.055  0.455  0.235  0.745\n",
       "4        0.000  0.135  0.055  0.190\n",
       "All      0.065  0.645  0.290  1.000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz = pd.crosstab(nike_limpao[\"Níveis\"], nike_limpao[\"Chute\"], rownames=[\"Real\"], colnames=[\"Predito\"], margins=True, normalize=True)\n",
    "matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentagem de muitos relevantes verdadeiros: 0%\n",
      "Porcentagem de relevantes verdadeiros: 0%\n",
      "Porcentagem de neutros verdadeiros: 1.0%\n",
      "Porcentagem de relevantes verdadeiros: 45.5%\n",
      "Porcentagem de muito relevante falsos: 5.5%\n"
     ]
    }
   ],
   "source": [
    "#calculando a probabilidade de positivos verdadeiros, positivos falsos, negativos verdadeiros e negativos falsos totais do classificador\n",
    "\n",
    "print(\"Porcentagem de muitos relevantes verdadeiros: 0%\")\n",
    "print(\"Porcentagem de relevantes verdadeiros: 0%\")\n",
    "print(\"Porcentagem de neutros verdadeiros: {0}%\".format((matriz.loc[2, 2])*100))\n",
    "print(\"Porcentagem de relevantes verdadeiros: {0}%\".format((matriz.loc[3, 3])*100))\n",
    "print(\"Porcentagem de muito relevante falsos: {0}%\".format((matriz.loc[4, 4])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Análise dos dados: **\n",
    "<p>Note-se de que antes de realizarmos os cinco diferentes níveis de comportamento de um twitter, iteramos primeiramente com dois níveis sendo relevante ou irrelevante. Realizamos a mesma programação usada para este. Porém, como são apenas duas alternativas, a chance da porcentagem de ser positivos verdadeiros e os negativos verdadeiros são maiores. Sendo assim, nossa programação da primeira iteração deu </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'porcentagem.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-0beef0b80a5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'porcentagem.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m         super(Image, self).__init__(data=data, url=url, filename=filename, \n\u001b[1;32m-> 1134\u001b[1;33m                 metadata=metadata)\n\u001b[0m\u001b[0;32m   1135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'width'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[0;32m    606\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36mreload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1163\u001b[0m         \u001b[1;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1164\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1165\u001b[1;33m             \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1166\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36mreload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 633\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    634\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'porcentagem.png'"
     ]
    }
   ],
   "source": [
    "Image(filename='porcentagem.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "** Possíveis otimizações : **\n",
    "\n",
    "<p>Para implementar e aumentar a confiabilidade do classificador, seria viável colher mais tweets para testes, levando em conta também o contexto no qual foram compartilhados, tendo em vista o alto número de retweets, gifs e imagens. Como o classificador ainda não consegue ler e interpretar imagens, uma opção seria usar o url fonte da imagem ou gif, afim de tentar analisar o contexto no qual a imagem está inserida.</p>\n",
    "\n",
    "<p> Outra maneira de melhorar o classificador de Naive Bayes seria implementando um código capaz de reconhecer e analisar o sarcasmo e comentários de dupla negação. Para tal, a leitura de emojis pelo método do Kernel Gaussiano seria essencial e de extrema relevância.</p>\n",
    "\n",
    "<p>O MVP desenvolvido foca na classificação de tweets de forma a destacar aqueles que podem ser mais relevantes para uma dada empresa, no caso a Nike, tendo em vista que muitas das postagens não fazem qualquer crítica, sugestão ou comentário que mereça atenção por não afetar positivamente ou mesmo negativamente a marca. Dessa forma, uma possível iteração para otimização na classificação de tweets seria a categorização de mais níveis que expressem mais especificamente a intenção na publicação de um tweet. Em se tratando de uma especificação de positivos e negativos, é necessário ressaltar que algumas palavras contidas na mensagem podem determinar o tom da crítica e são facilmente identificadas na base de treinamento por conta do contexto, como por exemplo palavras de negação (não, nunca, never, not, can’t). Por outro lado, na validação do programa ao rodar a planilha de testes podem ocorrer alguns conflitos e gerar interpretações contrários por conta das mesmas palavras. Uma sugestão é, para qualquer palavra que estiver entre uma palavra de negação e o fim do tweet ou um sinal de pontuação, pode ser classificada como negativa. Por exemplo, uma frase do tipo: “That sneaker was not the best thing I’ve ever wore” pode ser classificada erroneamente como positiva por conter a palavra best, mas se ambas analisadas juntas, sendo “best” seguindo de not, ela passa a ser então classificada corretamente. Pode-se programar isso de forma que o classificador reconheça se uma palavra vem antes da outra ao comparar seus indices em uma lista</p>\n",
    "\n",
    "<p>O classificador que estamos utilizando no desenvolvimento do MVP, o Naive Bayes, é um algoritmo de aprendizado de máquina supervisionado, ou seja, seu aprendizado é feito usando uma verdade básica, ou em outras palavras, temos conhecimento prévio de quais devem ser os valores de saída para nossas amostras. Em outros machine learnings, uma maneira de fazer com que não fosse preciso avaliar fazer manualmente cada tweet para poder fazer a programação , teria que fazer uma espécie de memória para que os tweets pudessem ser analisados automaticamente, sem precisar fazer manualmente. Por exemplo, como a memória de uma pessoa: ao ser alfabetizado, ao ser ensinado que de que a junção das letras \"m\" e \"o\"  formam \"mo\", ao ler depois em uma palavra, a criança lembrará como pronunciar tal.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências:\n",
    "<p>http://cs229.stanford.edu/proj2015/044_report.pdf </p>\n",
    "<p>https://www.sciencedirect.com/science/article/pii/S0160791X16300070</p>\n",
    "<p>https://blog.easysol.net/machine-learning-algorithms-4/</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
