{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "># **Ciência dos Dados** <font color=green>- Projeto 2</font>\n",
    "   #### <font color=grey> Beatriz Mie Kotsubo Kuwabara, <p>Lucas Nicascio dos Santos,</p>Manuela Castilla Russo Correa <p> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificador Automático de Sentimento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classificação de informação se dá pela categorização de uma grande base de dados. Atualmente, uma grande fonte de dados e consequentemente de informações que ajudam a compreender comportamentos e tendências da sociedade são as redes sociais, como o Twitter. Nesse projeto, utilizando-se da API do Twitter, tivemos acesso a alguns recentes tweets publicados em inglês na plataforma de forma a analisar as reações dos usuários à marca Nike ultimamente dado o lançamento de uma recente campanha protagonizada pelo ex-jogador da NFL Colin Kaepernick, gerando reações contra e a favor à marca. Na base da dados foi classificado se os tweets eram ou não relevantes à Nike bem como se demosntravam ser positivos ou negativos. \n",
    "\n",
    "<p>A implementação do \"machine learning\" por meio do classificador Naive Bayes utiliza-se de uma base de 200 tweets para o treinamento do código para uma posterior verificação com diferentes tweets em uma base de teste.</p>\n",
    "\n",
    "<p>O classificador Naive Bayes é um algoritmo para a tarefa de classificação utilizadondo o teorema de Bayes, que é uma equação que descreve a relação de probabilidades condicionais de grandezas estatísticas. O classificador fornece ótimos resultados quando o usamos para análise de dados textuais.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'emoji'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2a968deb88c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0memoji\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mUNICODE_EMOJI\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'emoji'"
     ]
    }
   ],
   "source": [
    "# Primeiramente importamos as bibliotecas para poder realizar o código\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as math\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from emoji import UNICODE_EMOJI\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura dos arquivos em Excel\n",
    "nike = pd.read_excel('tweets_nike_201809042210.xlsx')\n",
    "nike_treinamento = nike[\"Treinamento\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remoção de virgula\n",
    "sopa = \"  \".join(list(nike.Treinamento))\n",
    "split_sopa = sopa.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'UNICODE_EMOJI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-00ac21ddedea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mUNICODE_EMOJI\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpont\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'UNICODE_EMOJI' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "pontu = string.punctuation\n",
    "\n",
    "# Remove pontuação\n",
    "#table = str.maketrans('', '', string.punctuation)\n",
    "#pelado = [w.translate(table) for w in split_sopa]\n",
    "#print(pelado)\n",
    "\n",
    "pont =[\"\\n\", \"\\t\"]\n",
    "\n",
    "tweet = nike[\"Treinamento\"]\n",
    "\n",
    "tweets_limpos = []\n",
    "\n",
    "for e in tweet:\n",
    "    x = \"\"\n",
    "    for m in e:\n",
    "        if m in UNICODE_EMOJI:\n",
    "            x = x + \" \" + m + \" \"\n",
    "        elif m in pont:\n",
    "            x += \" \"\n",
    "        elif m not in pontu:\n",
    "            x += m\n",
    "    tweets_limpos.append(x)\n",
    "\n",
    "nike_limpinho = pd.DataFrame()\n",
    "nike_limpinho[\"tweets\"]= tweets_limpos\n",
    "nike_limpinho[\"Relevância\"] = nike[\"Valor_(R(1)_I(0))\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nike_limpinho' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-22ad3be2ad8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtweet_limpos\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnike_limpinho\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tweets\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mlimpao\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0msplitei\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nike_limpinho' is not defined"
     ]
    }
   ],
   "source": [
    "## limpar os nomes\n",
    "\n",
    "tweet_limpos =[]\n",
    "y = \" \"\n",
    "for e in nike_limpinho[\"tweets\"]:\n",
    "    limpao = [] \n",
    "    splitei = e.split(\" \")\n",
    "    for m in splitei:\n",
    "        if m in UNICODE_EMOJI:\n",
    "            limpao.append(m)\n",
    "        elif len(m)> 2 and m[0]!=\"@\" and m[0]!=\"#\" and m[:4] !=\"http\":\n",
    "            limpao.append(m)\n",
    "    tweet_limpos.append(y.join(limpao))\n",
    "    \n",
    "nike_limpao = pd.DataFrame()\n",
    "nike_limpao[\"tweets\"]= tweet_limpos\n",
    "nike_limpao[\"Relevância\"] = nike[\"Valor_(R(1)_I(0))\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras = []\n",
    "\n",
    "for e in nike_limpao[\"tweets\"]: \n",
    "    y = e.split()\n",
    "    for m in y:\n",
    "        if m not in palavras: \n",
    "            palavras.append(m)\n",
    "            \n",
    "Rel = 0\n",
    "Irrel = 0\n",
    "for i in range(len(nike_limpao)):\n",
    "    linha = nike_limpao[\"tweets\"][i].split(\" \")\n",
    "    for m in linha:\n",
    "        if nike_limpao[\"Relevância\"][i] == 0:\n",
    "            Irrel +=1\n",
    "        else:\n",
    "            Rel +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_rel = {}\n",
    "freq_irrel = {}\n",
    "\n",
    "for palavra in palavras:\n",
    "    freq_rel[palavra] = 1\n",
    "    freq_irrel[palavra] = 1\n",
    "\n",
    "for i in range(len(nike_limpao)):\n",
    "    palavra = nike_limpao[\"tweets\"][i].split(\" \")\n",
    "    for m in palavra:\n",
    "        if nike_limpao[\"Relevância\"][i]== 0:\n",
    "            freq_irrel[m]+=1\n",
    "        else:\n",
    "            freq_rel[m]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prob_rel = probabilidade de ser relevante dado tal palavra\n",
    "prob_rel={}\n",
    "prob_irrel={}\n",
    "\n",
    "for palavra in palavras:\n",
    "    prob_rel[palavra] = freq_rel[palavra]/(len(palavras)+Rel)\n",
    "    prob_irrel[palavra] = freq_irrel[palavra]/(len(palavras)+ Irrel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Contagem de tweets relevantes e irrelevantes e suas probabilidades\n",
    "\n",
    "irrelevante = 0\n",
    "relevante = 0\n",
    "for i in nike_limpao[\"Relevância\"]:\n",
    "    if i == 0:\n",
    "        irrelevante+=1\n",
    "    else: \n",
    "        relevante +=1\n",
    "\n",
    "pI = irrelevante/len(nike_limpao[\"Relevância\"])\n",
    "pR = relevante/len(nike_limpao[\"Relevância\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'float' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-9a2849bb2059>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpalavra\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mpalavra\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprob_rel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpalavra\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mmultiplicador\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiplicador\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprob_rel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpalavra\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'float' is not iterable"
     ]
    }
   ],
   "source": [
    "## naive-bayes\n",
    "multiplicador = 1\n",
    "for frase in nike_limpao[\"tweets\"]:\n",
    "    y = frase.split()\n",
    "    for palavra in y:\n",
    "        if palavra in prob_rel[palavra]:\n",
    "            multiplicador = multiplicador * prob_rel[palavra]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## referências\n",
    "https://machinelearningmastery.com/clean-text-machine-learning-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
